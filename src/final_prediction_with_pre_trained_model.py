# -*- coding: utf-8 -*-
"""Final prediction with pre-trained model.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/14CNGPetFeyaAeDWb2jWuk0mRx9QdPv8s
"""

# terminal installs
#pip install efficientnet_pytorch

# execute only in dataset is not extracted into inputs from drive in account 20p244@kce.ac.in

#from google.colab import drive
#import zipfile

#drive.mount('/content/drive', force_remount=True)

#!rm -rf /content/models
#!rm -rf /content/scripts
#!cp -r /content/drive/MyDrive/DataSet/models /content/models
#!cp -r /content/drive/MyDrive/DataSet/scripts /content/scripts

#zip_ref = zipfile.ZipFile("/content/drive/MyDrive/DataSet/archive.zip", 'r')
#zip_ref.extractall("/input/aptos2019-blindness-detection/")
#zip_ref.close()

#zip_ref = zipfile.ZipFile("/content/drive/MyDrive/DataSet/archive1.zip", 'r')
#zip_ref.extractall("/input/diabetic-retinopathy-resized/")
#zip_ref.close()

"""# PREPARATIONS"""

# Commented out IPython magic to ensure Python compatibility.
##### LIBRARIES

import numpy as np
import pandas as pd

import torch
import torchvision

import torch.nn as nn
import torch.optim as optim
from torch.optim import lr_scheduler
from torchvision import transforms, models, datasets
from torch.utils.data import Dataset
from torch.autograd import Variable

from PIL import Image, ImageFile
ImageFile.LOAD_TRUNCATED_IMAGES = True
import cv2

from sklearn import metrics
from sklearn.metrics import confusion_matrix
from sklearn.model_selection import KFold, StratifiedKFold

from tqdm import tqdm_notebook as tqdm
from functools import partial
import scipy as sp

import random
import time
import sys
import os
import math

from efficientnet_pytorch import EfficientNet

import matplotlib.pyplot as plt
import seaborn as sns
# %matplotlib inline

import warnings
warnings.filterwarnings('ignore')

##### CUSTOM MODULES
print('Import done')

sys.path.append('./') # need change
from preprocessing import *
from data import EyeTestData
from utilities import seed_everything
from model import init_model

# ##### GPU CHECK
# print('GPU check')

# train_on_gpu = torch.cuda.is_available()
# if not train_on_gpu:
#     print('CUDA is not available. Training on CPU...')
#     device = torch.device('cpu')
# else:
#     print('CUDA is available. Training on GPU...')
#     device = torch.device('cuda:0')

print('CUDA is not available. Training on CPU...')
device = torch.device('cpu')

##### RANDOMNESS

# seed function
def seed_everything(seed = 23):
    os.environ['PYTHONHASHSEED'] = str(seed)
    random.seed(seed)
    np.random.seed(seed)
    torch.manual_seed(seed)
    torch.cuda.manual_seed(seed)
    torch.backends.cudnn.deterministic = True
    torch.backends.cudnn.benchmark = False

# set seed
seed = 23
seed_everything(seed)

"""# DATA IMPORT"""

##### CHECK DIMENSIONS

# import data

databuff = []

if len(sys.argv) < 1:
    print("Usage: python script.py <argument>")
    sys.exit(1)

for argument in sys.argv[1:]:
    databuff.append([argument,0])

test = pd.DataFrame(databuff, columns=['id_code','diagnosis'])


# check shape
print(test.shape)

##### TRANSFORMATIONS

# parameters
batch_size = 25
image_size = 256

# test transformations
test_trans = transforms.Compose([transforms.ToPILImage(),
                                  transforms.RandomHorizontalFlip(),
                                  transforms.RandomVerticalFlip(),
                                  transforms.ToTensor()
                                 ])

##### DATA LOADER

# create dataset
test_dataset = EyeTestData(data      = test,
                           directory = './uploads/', # need change
                           transform = test_trans)

# create data loader
test_loader = torch.utils.data.DataLoader(test_dataset,
                                          batch_size  = batch_size,
                                          shuffle     = False,
                                          num_workers = 4)

"""# DATA CHECK"""

##### EXAMINE THE FIRST BATCH

# # display images
# for batch_i, data in enumerate(test_loader):

#     # extract data
#     inputs = data['image']

#     # create plot
#     fig = plt.figure(figsize = (14, 7))
#     for i in range(1):
#         ax = fig.add_subplot(2, int(10/2), i + 1, xticks = [], yticks = [])
#         plt.imshow(inputs[i].numpy().transpose(1, 2, 0))

#     break

"""# MODEL SETUP"""

##### MODEL ARCHITECTURE

# model name
model_name = 'enet_b4'

# check architecture
model = init_model(train = False, model_name = 'enet_b4')
# print(model)

"""# INFERENCE LOOP"""

##### INFERENCE LOOP

# validation settings
num_folds = 4
tta_times = 4

# placeholders
test_preds = np.zeros((len(test), num_folds))
cv_start   = time.time()

# prediction loop
for fold in tqdm(range(num_folds)):

    # load model and sent to GPU
    model = init_model(model_name = 'enet_b4', train = False)
    model.load_state_dict(torch.load('/media/shiddesh/MY FILES/docker/Diabetic retinopathy image/model/model_{}_fold{}.bin'.format(model_name, fold + 1), map_location=torch.device('cpu')))  # need change
    model = model.to(device)
    model.eval()

    # placeholder
    fold_preds = np.zeros((len(test), 1))

    # loop through batches
    for _ in range(tta_times):
        for batch_i, data in enumerate(test_loader):
            inputs = data['image']
            inputs = inputs.to(device, dtype = torch.float)
            preds = model(inputs).detach()
            _, class_preds = preds.topk(1)
            fold_preds[batch_i * batch_size:(batch_i + 1) * batch_size, :] += class_preds.cpu().numpy()
    fold_preds = fold_preds / tta_times

    # aggregate predictions
    test_preds[:, fold] = fold_preds.reshape(-1)

# print performance
test_preds_df = pd.DataFrame(test_preds.copy())
print('Finished in {:.2f} minutes'.format((time.time() - cv_start) / 60))

##### SUMMARIZE PREDICTIONS

# show predictions
print('-' * 45)
print('PREDICTIONS')
print('-' * 45)
print(test_preds_df.head())

# show correlation
print('-' * 45)
print('CORRELATION MATRIX')
print('-' * 45)
print(np.round(test_preds_df.corr(), 4))
print('Mean correlation = ' + str(np.round(np.mean(np.mean(test_preds_df.corr())), 4)))

# show stats
print('-' * 45)
print('SUMMARY STATS')
print('-' * 45)
print(test_preds_df.describe())

# show prediction distribution
print('-' * 45)
print('ROUNDED PREDICTIONS')
print('-' * 45)
for f in range(num_folds):
    print(np.round(test_preds_df[f]).astype('int').value_counts(normalize = True))
    print('-' * 45)

# plot densities
test_preds_df.plot.kde()

##### AGGREGATE PREDICTIONS

# extract prediction
test_preds = test_preds_df.mean(axis = 1).values

"""# SAVE PREDICTIONS"""

##### THRESHOLD OPTIMIZER

# load cutoffs
coef = [0.5, 1.75, 2.25, 3.5]

# rounding
for i, pred in enumerate(test_preds):
    if pred < coef[0]:
        test_preds[i] = 0
    elif pred >= coef[0] and pred < coef[1]:
        test_preds[i] = 1
    elif pred >= coef[1] and pred < coef[2]:
        test_preds[i] = 2
    elif pred >= coef[2] and pred < coef[3]:
        test_preds[i] = 3
    else:
        test_preds[i] = 4

##### EXPORT CSV

# construct data frame
sub = pd.DataFrame(databuff, columns=['id_code','diagnosis'])# need change
sub['diagnosis'] = test_preds.astype('int')

# save predictions
sub.to_json('result.json') # need change

##### CHECK DISTRIBUTION

# print frequencies
sub['diagnosis'].value_counts(normalize = True)
